{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VAcsZ7XxW7XL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from patchify import (patchify, unpatchify)\n",
    "from utils import center_crop, rebuild_from_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a0lOP-N2mp6W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Models\n",
      "--------------\n",
      "0 --> unet_input512_filters32_images160_independent_patches_optAdam_lr0.0010_eps150\n",
      "1 --> unet_input512_filters32_images160_independent_patches_optAdam_lr0.0010_eps500\n",
      "2 --> unet_input512_filters32_images160_random_patches_optAdam_lr0.0010_eps150\n",
      "3 --> unet_input512_filters32_images160_random_patches_optAdam_lr0.0010_eps500\n",
      "4 --> unet_input512_filters32_images589_overlapped_patches_optAdam_lr0.0010_eps150\n",
      "5 --> unet_input512_filters32_images589_overlapped_patches_optAdam_lr0.0010_eps500\n",
      "6 --> unet_input512_filters32_images589_random_patches_optAdam_lr0.0010_eps150\n",
      "7 --> unet_input512_filters32_images589_random_patches_optAdam_lr0.0010_eps500\n",
      "\n",
      "Choose a model from the list by typing the number of its key + Enter:\n",
      "5\n",
      "\n",
      "unet_input512_filters32_images589_overlapped_patches_optAdam_lr0.0010_eps500 was selected.\n"
     ]
    }
   ],
   "source": [
    "# Display a list with the available models and ask the user to choose which to use\n",
    "models_path = 'models/f001_w_val_data'\n",
    "models_list = [name for name in os.listdir(models_path) if os.path.isdir(os.path.join(models_path, name))]\n",
    "if 'logs' in models_list:\n",
    "    models_list.remove('logs')\n",
    "\n",
    "# create a dictionary with numbers as keys and the model names as values\n",
    "models2dict = dict([(str(a), b) for a, b in enumerate(models_list)])\n",
    "\n",
    "print('List of Models', '--------------', sep='\\n')\n",
    "for item in models2dict.items():\n",
    "    print(*item, sep=' --> ')\n",
    "\n",
    "print('\\nChoose a model from the list by typing the number of its key + Enter:')\n",
    "model_name = models2dict[input()]\n",
    "print(f'\\n{model_name} was selected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2x5kHDiEGt4k"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = keras.models.load_model(os.path.join(models_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bV6_F_A-I4hc"
   },
   "outputs": [],
   "source": [
    "# Retrieve the input layer and extract the size of width of the images (which is equal to the height)\n",
    "input_layer = model.get_layer(index=0)\n",
    "input_size = input_layer.input_shape[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_fiLgoAGVqKp"
   },
   "outputs": [],
   "source": [
    "# Load the input and ground truth images \n",
    "x_initial_test = plt.imread('test/301-INPUT.jpg')\n",
    "y_initial_test = plt.imread('test/301-OUTPUT-GT.png')\n",
    "\n",
    "CROP_SIZE_W = 4000 \n",
    "CROP_SIZE_H = 4000\n",
    "\n",
    "x_initial_test = center_crop(x_initial_test, (CROP_SIZE_H, CROP_SIZE_W))\n",
    "y_initial_test = center_crop(y_initial_test, (CROP_SIZE_H, CROP_SIZE_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SOPVSWF23qJY",
    "outputId": "fbd75ecc-84cb-4397-f327-31624905ab03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 4000, 3)\n",
      "(4000, 4000)\n"
     ]
    }
   ],
   "source": [
    "print(x_initial_test.shape)\n",
    "print(y_initial_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rjc-22iiWKRc"
   },
   "outputs": [],
   "source": [
    "# Create patches from the input and ground truth images\n",
    "STEP = 128\n",
    "input_patches = np.squeeze(patchify(x_initial_test, (input_size, input_size, 3), step=STEP))\n",
    "ground_truth_patches = np.squeeze(patchify(y_initial_test, (input_size, input_size), step=STEP))\n",
    "print(input_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Lrx9wAYwJdn",
    "outputId": "25201931-c642-48aa-8d14-bc76683b7cef"
   },
   "outputs": [],
   "source": [
    "predictions = np.squeeze(model.predict(np.reshape(input_patches, (-1, input_size, input_size, 3))))\n",
    "predictions = np.reshape(predictions, ground_truth_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0NckwATdsjb",
    "outputId": "9b4fe9a8-b8fe-49c4-9a36-0a39a340e432"
   },
   "outputs": [],
   "source": [
    "overlapped_images = int(np.power((input_size / STEP), 2))\n",
    "unified_predictions = rebuild_from_patches(predictions, y_initial_test.shape, STEP, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLh5PBZBdsjc",
    "outputId": "e7e23553-a36b-4522-84d4-0aa220b07cd9"
   },
   "outputs": [],
   "source": [
    "plot_side = int(np.sqrt(unified_predictions.shape[-1]))\n",
    "\n",
    "fig, ax = plt.subplots(plot_side, plot_side, dpi=300)\n",
    "i = 0\n",
    "for row in range(plot_side):\n",
    "    for col in range(plot_side):\n",
    "        ax[row, col].imshow(unified_predictions[:, :, i])\n",
    "        ax[row, col].set_xticks([])\n",
    "        ax[row, col].set_yticks([])\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = center_crop(x_initial_test, (unified_predictions.shape[:2])) / 255. #NOTE: Later, I must divide by 255 the training image, not here\n",
    "y = np.expand_dims(center_crop(y_initial_test, (unified_predictions.shape[:2])), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZgGRDkzdsje"
   },
   "outputs": [],
   "source": [
    "x_and_y_and_predictions = np.concatenate([x, y, unified_predictions], -1)\n",
    "\n",
    "name = \"prediction_\" + model_name\n",
    "np.save(os.path.join(models_path, name), x_and_y_and_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "net_eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:deepenv] *",
   "language": "python",
   "name": "conda-env-deepenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
