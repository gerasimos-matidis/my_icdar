{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAcsZ7XxW7XL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from patchify import (patchify, unpatchify)\n",
    "from utils import center_crop, rebuild_from_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0lOP-N2mp6W"
   },
   "outputs": [],
   "source": [
    "# Display a list with the available models and ask the user to choose which to use\n",
    "models_path = 'models/f001b'\n",
    "models_list = [name for name in os.listdir(models_path) if os.path.isdir(os.path.join(models_path, name))]\n",
    "if 'logs' in models_list:\n",
    "    models_list.remove('logs')\n",
    "if 'predictions' in models_list:\n",
    "    models_list.remove('predictions')\n",
    "\n",
    "# create a dictionary with numbers as keys and the model names as values\n",
    "models2dict = dict([(str(a), b) for a, b in enumerate(models_list)])\n",
    "\n",
    "print('List of Models', '--------------', sep='\\n')\n",
    "for item in models2dict.items():\n",
    "    print(*item, sep=' --> ')\n",
    "\n",
    "print('\\nChoose a model from the list by typing the number of its key + Enter:')\n",
    "model_name = models2dict[input()]\n",
    "print(f'\\n{model_name} was selected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x5kHDiEGt4k"
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = keras.models.load_model(os.path.join(models_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV6_F_A-I4hc"
   },
   "outputs": [],
   "source": [
    "# Retrieve the input layer and extract the size of width of the images (which is equal to the height)\n",
    "input_layer = model.get_layer(index=0)\n",
    "input_size = input_layer.input_shape[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fiLgoAGVqKp"
   },
   "outputs": [],
   "source": [
    "# Load the input and ground truth images \n",
    "x_initial_test = plt.imread('test/303-INPUT.jpg')\n",
    "y_initial_test = plt.imread('test/303-OUTPUT-GT.png')\n",
    "\n",
    "dest_foldername = 'dataset_303'\n",
    "\n",
    "CROP_SIZE_W = 4000 \n",
    "CROP_SIZE_H = 4000\n",
    "\n",
    "x_initial_test = center_crop(x_initial_test, (CROP_SIZE_H, CROP_SIZE_W))\n",
    "y_initial_test = center_crop(y_initial_test, (CROP_SIZE_H, CROP_SIZE_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOPVSWF23qJY",
    "outputId": "fbd75ecc-84cb-4397-f327-31624905ab03"
   },
   "outputs": [],
   "source": [
    "print(x_initial_test.shape)\n",
    "print(y_initial_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rjc-22iiWKRc"
   },
   "outputs": [],
   "source": [
    "# Create patches from the input and ground truth images\n",
    "STEP = 128\n",
    "input_patches = np.squeeze(patchify(x_initial_test, (input_size, input_size, 3), step=STEP))\n",
    "ground_truth_patches = np.squeeze(patchify(y_initial_test, (input_size, input_size), step=STEP))\n",
    "print(input_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Lrx9wAYwJdn",
    "outputId": "25201931-c642-48aa-8d14-bc76683b7cef"
   },
   "outputs": [],
   "source": [
    "predictions = np.squeeze(model.predict(np.reshape(input_patches, (-1, input_size, input_size, 3))))\n",
    "predictions = np.reshape(predictions, ground_truth_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0NckwATdsjb",
    "outputId": "9b4fe9a8-b8fe-49c4-9a36-0a39a340e432"
   },
   "outputs": [],
   "source": [
    "overlapped_images = int(np.power((input_size / STEP), 2))\n",
    "unified_predictions = rebuild_from_patches(predictions, y_initial_test.shape, STEP, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = center_crop(x_initial_test, (unified_predictions.shape[:2])) / 255. #NOTE: Later, I must divide by 255 the training image, not here\n",
    "y = np.expand_dims(center_crop(y_initial_test, (unified_predictions.shape[:2])), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZgGRDkzdsje"
   },
   "outputs": [],
   "source": [
    "x_and_y_and_predictions = np.concatenate([x, y, unified_predictions], -1)\n",
    "\n",
    "dest = models_path + '/predictions/' + dest_foldername\n",
    "if not os.path.isdir(dest):\n",
    "    os.makedirs(dest)\n",
    "name = \"prediction_\" + model_name\n",
    "np.save(os.path.join(dest, name), x_and_y_and_predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "net_eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c11aab322e2a22a0ab3f45ca9006482b0337b6102f0db376cd2f32a3f939d985"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
