{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xnMOsbqHz61"
   },
   "source": [
    "# pix2pix: Image-to-image translation with a conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:30:49.942405Z",
     "iopub.status.busy": "2022-01-26T04:30:49.941819Z",
     "iopub.status.idle": "2022-01-26T04:30:52.426874Z",
     "shell.execute_reply": "2022-01-26T04:30:52.427267Z"
    },
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "PATCHES_METHOD = 'independent_patches'   # Choose between independent_patches, overlapped_patches an random_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'D:/Gerasimos/my_icdar/datasets'\n",
    "train_set_path = os.path.join(PATH, f'train/RGB/{IMG_WIDTH}by512/{PATCHES_METHOD}/101' )\n",
    "test_set_path = os.path.join(PATH, f'test/RGB/{IMG_WIDTH}by512/independent_patches/301_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.440472Z",
     "iopub.status.busy": "2022-01-26T04:31:14.439360Z",
     "iopub.status.idle": "2022-01-26T04:31:14.441597Z",
     "shell.execute_reply": "2022-01-26T04:31:14.442405Z"
    },
    "id": "2CbTEt448b4R"
   },
   "outputs": [],
   "source": [
    "# The buffer size is set to be equal with the total number of the training images\n",
    "buffer_size = len(os.listdir(train_set_path))\n",
    "test_set_num = len(os.listdir(test_set_path))\n",
    "steps_per_epoch = int(buffer_size / BATCH_SIZE)\n",
    "steps = buffer_size * EPOCHS # TODO: Delete it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'D:/Gerasimos/my_icdar/models/{IMG_WIDTH}_inputs/lr0002_aug/pix2pix'\n",
    "gen_name = model_name = f'pix2pix_input{IMG_WIDTH}_filters64_images{buffer_size}_{PATCHES_METHOD}_optAdam_lr0.0002_eps{EPOCHS}' #TODO: loss and metric'\n",
    "gen_path = os.path.join(model_path, gen_name)\n",
    "checkpoint_dir = os.path.join(gen_path, f'training_checkpoints_{PATCHES_METHOD}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.009760Z",
     "iopub.status.busy": "2022-01-26T04:31:14.009099Z",
     "iopub.status.idle": "2022-01-26T04:31:14.013316Z",
     "shell.execute_reply": "2022-01-26T04:31:14.012915Z"
    },
    "id": "XGY1kiptguTQ"
   },
   "outputs": [],
   "source": [
    "sample_image = tf.io.read_file(str(train_set_path  + '/8.jpg'))\n",
    "sample_image = tf.io.decode_jpeg(sample_image)\n",
    "print(sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.019631Z",
     "iopub.status.busy": "2022-01-26T04:31:14.015776Z",
     "iopub.status.idle": "2022-01-26T04:31:14.170812Z",
     "shell.execute_reply": "2022-01-26T04:31:14.171200Z"
    },
    "id": "vJ2sO8Izg7QV"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(sample_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2A5SU-qxPAqd"
   },
   "source": [
    "You need to separate real building facade images from the architecture label images—all of which will be of size `256 x 256`.\n",
    "\n",
    "Define a function that loads image files and outputs two image tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.177170Z",
     "iopub.status.busy": "2022-01-26T04:31:14.176369Z",
     "iopub.status.idle": "2022-01-26T04:31:14.178304Z",
     "shell.execute_reply": "2022-01-26T04:31:14.178691Z"
    },
    "id": "aO9ZAGH5K3SY"
   },
   "outputs": [],
   "source": [
    "def load(image_file):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = tf.io.read_file(image_file)\n",
    "    image = tf.io.decode_jpeg(image)\n",
    "\n",
    "    # Split each image tensor into two tensors:\n",
    "    # - one with a real building facade image\n",
    "    # - one with an architecture label image \n",
    "    w = tf.shape(image)[1]\n",
    "    w = w // 2\n",
    "    input_image = image[:, :w, :] # NOTE:  !!!!!!! Needs to change\n",
    "    #target_image = image[:, :w, :]\n",
    "    target_image = image[:, w:, 0] # Gerasimos changed it, because the target image is binary\n",
    "    # Convert both images to float32 tensors\n",
    "    input_image = tf.cast(input_image, tf.float32)\n",
    "    target_image = tf.cast(target_image, tf.float32)\n",
    "\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5ByHTlfE06P"
   },
   "source": [
    "Plot a sample of the input (architecture label image) and real (building facade photo) images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.183566Z",
     "iopub.status.busy": "2022-01-26T04:31:14.182767Z",
     "iopub.status.idle": "2022-01-26T04:31:14.433716Z",
     "shell.execute_reply": "2022-01-26T04:31:14.434453Z"
    },
    "id": "4OLHMpsQ5aOv"
   },
   "outputs": [],
   "source": [
    "inp, tar = load(str(train_set_path + '/92.jpg'))\n",
    "# Casting to int for matplotlib to display the images\n",
    "plt.figure()\n",
    "plt.imshow(inp / 255.0)\n",
    "plt.figure()\n",
    "plt.imshow(tar / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVuZQTfI_c-s"
   },
   "source": [
    "As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n",
    "\n",
    "Define several functions that:\n",
    "\n",
    "1. Resize each `256 x 256` image to a larger height and width—`286 x 286`.\n",
    "2. Randomly crop it back to `256 x 256`.\n",
    "3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n",
    "4. Normalize the images to the `[-1, 1]` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.448439Z",
     "iopub.status.busy": "2022-01-26T04:31:14.447110Z",
     "iopub.status.idle": "2022-01-26T04:31:14.449532Z",
     "shell.execute_reply": "2022-01-26T04:31:14.450153Z"
    },
    "id": "rwwYQpu9FzDu"
   },
   "outputs": [],
   "source": [
    "def resize(input_image, target_image, height, width):\n",
    "    input_image = tf.image.resize(input_image, [height, width],\n",
    "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    target_image = tf.expand_dims(target_image, axis=-1)\n",
    "    target_image = tf.image.resize(target_image, [height, width],\n",
    "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.455874Z",
     "iopub.status.busy": "2022-01-26T04:31:14.454822Z",
     "iopub.status.idle": "2022-01-26T04:31:14.456775Z",
     "shell.execute_reply": "2022-01-26T04:31:14.457218Z"
    },
    "id": "Yn3IwqhiIszt"
   },
   "outputs": [],
   "source": [
    "def random_crop(input_image, real_image):\n",
    "    stacked_image = tf.concat([input_image, real_image], -1)\n",
    "    cropped_image = tf.image.random_crop(\n",
    "        stacked_image, size=[IMG_HEIGHT, IMG_WIDTH, 4])\n",
    "\n",
    "    return cropped_image[:, :, :3], tf.expand_dims(cropped_image[:, :, 3], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.461165Z",
     "iopub.status.busy": "2022-01-26T04:31:14.460647Z",
     "iopub.status.idle": "2022-01-26T04:31:14.462534Z",
     "shell.execute_reply": "2022-01-26T04:31:14.462884Z"
    },
    "id": "muhR2cgbLKWW"
   },
   "outputs": [],
   "source": [
    "# Normalizing the images to [-1, 1]\n",
    "def normalize(input_image, real_image):\n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.467702Z",
     "iopub.status.busy": "2022-01-26T04:31:14.467144Z",
     "iopub.status.idle": "2022-01-26T04:31:14.469305Z",
     "shell.execute_reply": "2022-01-26T04:31:14.468907Z"
    },
    "id": "fVQOjcPVLrUc"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def random_jitter(input_image, target_image):\n",
    "    # Resizing to 286x286\n",
    "    input_image, target_image = resize(input_image, target_image, 286, 286)\n",
    "    \n",
    "    # Random cropping back to 256x256\n",
    "    input_image, target_image = random_crop(input_image, target_image)\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Random mirroring\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        target_image = tf.image.flip_left_right(target_image)\n",
    "\n",
    "    return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfAQbzy799UV"
   },
   "source": [
    "You can inspect some of the preprocessed output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.474423Z",
     "iopub.status.busy": "2022-01-26T04:31:14.473891Z",
     "iopub.status.idle": "2022-01-26T04:31:14.864972Z",
     "shell.execute_reply": "2022-01-26T04:31:14.865419Z"
    },
    "id": "n0OGdi6D92kM"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(4):\n",
    "    rj_inp, rj_tar = random_jitter(inp, tar)\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.imshow(rj_inp / 255.0)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E9LGq3WBmsh"
   },
   "source": [
    "Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.870487Z",
     "iopub.status.busy": "2022-01-26T04:31:14.869864Z",
     "iopub.status.idle": "2022-01-26T04:31:14.872135Z",
     "shell.execute_reply": "2022-01-26T04:31:14.871634Z"
    },
    "id": "tyaP4hLJ8b4W"
   },
   "outputs": [],
   "source": [
    "def load_image_train(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = random_jitter(input_image, real_image)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.876520Z",
     "iopub.status.busy": "2022-01-26T04:31:14.875976Z",
     "iopub.status.idle": "2022-01-26T04:31:14.878164Z",
     "shell.execute_reply": "2022-01-26T04:31:14.877739Z"
    },
    "id": "VB3Z6D_zKSru"
   },
   "outputs": [],
   "source": [
    "def load_image_test(image_file):\n",
    "    input_image, real_image = load(image_file)\n",
    "    input_image, real_image = resize(input_image, real_image,\n",
    "                                   IMG_HEIGHT, IMG_WIDTH)\n",
    "    input_image, real_image = normalize(input_image, real_image)\n",
    "\n",
    "    return input_image, real_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:14.882853Z",
     "iopub.status.busy": "2022-01-26T04:31:14.882034Z",
     "iopub.status.idle": "2022-01-26T04:31:15.041637Z",
     "shell.execute_reply": "2022-01-26T04:31:15.042042Z"
    },
    "id": "SQHmYSmk8b4b"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(train_set_path + '/*jpg')\n",
    "print(train_dataset)\n",
    "train_dataset = train_dataset.map(load_image_train,\n",
    "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:15.047230Z",
     "iopub.status.busy": "2022-01-26T04:31:15.046620Z",
     "iopub.status.idle": "2022-01-26T04:31:15.106920Z",
     "shell.execute_reply": "2022-01-26T04:31:15.106425Z"
    },
    "id": "MS9J0yA58b4g"
   },
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.list_files(test_set_path + '/*jpg')\n",
    "test_dataset = test_dataset.map(load_image_test)\n",
    "test_dataset = test_dataset.batch(test_set_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:15.117801Z",
     "iopub.status.busy": "2022-01-26T04:31:15.117283Z",
     "iopub.status.idle": "2022-01-26T04:31:15.119985Z",
     "shell.execute_reply": "2022-01-26T04:31:15.119523Z"
    },
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:15.126376Z",
     "iopub.status.busy": "2022-01-26T04:31:15.123706Z",
     "iopub.status.idle": "2022-01-26T04:31:16.561216Z",
     "shell.execute_reply": "2022-01-26T04:31:16.560753Z"
    },
    "id": "a6_uCZCppTh7"
   },
   "outputs": [],
   "source": [
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))\n",
    "print (down_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:16.567263Z",
     "iopub.status.busy": "2022-01-26T04:31:16.566711Z",
     "iopub.status.idle": "2022-01-26T04:31:16.568479Z",
     "shell.execute_reply": "2022-01-26T04:31:16.568797Z"
    },
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:16.573646Z",
     "iopub.status.busy": "2022-01-26T04:31:16.573126Z",
     "iopub.status.idle": "2022-01-26T04:31:16.759290Z",
     "shell.execute_reply": "2022-01-26T04:31:16.759680Z"
    },
    "id": "mz-ahSdsq0Oc"
   },
   "outputs": [],
   "source": [
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:15.111080Z",
     "iopub.status.busy": "2022-01-26T04:31:15.110503Z",
     "iopub.status.idle": "2022-01-26T04:31:15.112971Z",
     "shell.execute_reply": "2022-01-26T04:31:15.112548Z"
    },
    "id": "tqqvWxlw8b4l"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:16.768292Z",
     "iopub.status.busy": "2022-01-26T04:31:16.767729Z",
     "iopub.status.idle": "2022-01-26T04:31:16.769904Z",
     "shell.execute_reply": "2022-01-26T04:31:16.770230Z"
    },
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
    "\n",
    "    down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "    downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "    upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4PKwrcQFYvF"
   },
   "source": [
    "Visualize the generator model architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:16.776651Z",
     "iopub.status.busy": "2022-01-26T04:31:16.776051Z",
     "iopub.status.idle": "2022-01-26T04:31:17.541763Z",
     "shell.execute_reply": "2022-01-26T04:31:17.542225Z"
    },
    "id": "dIbRPFzjmV85"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:17.548529Z",
     "iopub.status.busy": "2022-01-26T04:31:17.547912Z",
     "iopub.status.idle": "2022-01-26T04:31:18.019412Z",
     "shell.execute_reply": "2022-01-26T04:31:18.019825Z"
    },
    "id": "U1N1_obwtdQH"
   },
   "outputs": [],
   "source": [
    "gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
    "plt.imshow(gen_output[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.024616Z",
     "iopub.status.busy": "2022-01-26T04:31:18.024020Z",
     "iopub.status.idle": "2022-01-26T04:31:18.026241Z",
     "shell.execute_reply": "2022-01-26T04:31:18.025767Z"
    },
    "id": "cyhxTuvJyIHV"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.030947Z",
     "iopub.status.busy": "2022-01-26T04:31:18.030306Z",
     "iopub.status.idle": "2022-01-26T04:31:18.031910Z",
     "shell.execute_reply": "2022-01-26T04:31:18.032257Z"
    },
    "id": "Q1Xbz5OaLj5C"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.037172Z",
     "iopub.status.busy": "2022-01-26T04:31:18.036346Z",
     "iopub.status.idle": "2022-01-26T04:31:18.038684Z",
     "shell.execute_reply": "2022-01-26T04:31:18.038175Z"
    },
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSZbDgESHIV6"
   },
   "source": [
    "The training procedure for the generator is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIuTeGL5v45m"
   },
   "source": [
    "Let's define the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.046613Z",
     "iopub.status.busy": "2022-01-26T04:31:18.045933Z",
     "iopub.status.idle": "2022-01-26T04:31:18.047839Z",
     "shell.execute_reply": "2022-01-26T04:31:18.048204Z"
    },
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "    down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "    down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "    down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "    conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.057701Z",
     "iopub.status.busy": "2022-01-26T04:31:18.055169Z",
     "iopub.status.idle": "2022-01-26T04:31:18.308704Z",
     "shell.execute_reply": "2022-01-26T04:31:18.309131Z"
    },
    "id": "YHoUui4om-Ev"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.317340Z",
     "iopub.status.busy": "2022-01-26T04:31:18.316691Z",
     "iopub.status.idle": "2022-01-26T04:31:18.587012Z",
     "shell.execute_reply": "2022-01-26T04:31:18.587439Z"
    },
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.592604Z",
     "iopub.status.busy": "2022-01-26T04:31:18.592011Z",
     "iopub.status.idle": "2022-01-26T04:31:18.594169Z",
     "shell.execute_reply": "2022-01-26T04:31:18.593595Z"
    },
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ede4p2YELFa"
   },
   "source": [
    "The training procedure for the discriminator is shown below.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS9sHa-1BoAF"
   },
   "source": [
    "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the optimizers and a checkpoint-saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.599142Z",
     "iopub.status.busy": "2022-01-26T04:31:18.598551Z",
     "iopub.status.idle": "2022-01-26T04:31:18.600410Z",
     "shell.execute_reply": "2022-01-26T04:31:18.600802Z"
    },
    "id": "lbHFNexF0x6O"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.605599Z",
     "iopub.status.busy": "2022-01-26T04:31:18.605034Z",
     "iopub.status.idle": "2022-01-26T04:31:18.607205Z",
     "shell.execute_reply": "2022-01-26T04:31:18.606744Z"
    },
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.613363Z",
     "iopub.status.busy": "2022-01-26T04:31:18.612674Z",
     "iopub.status.idle": "2022-01-26T04:31:18.614618Z",
     "shell.execute_reply": "2022-01-26T04:31:18.614986Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], np.where(prediction[0] > 0.5, 1, 0)]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(generator, discriminator, input_images, target_images):\n",
    "    gen_output = generator(input_images, training=True)\n",
    "\n",
    "    disc_real_output = discriminator([input_images, target_images], training=True)\n",
    "    disc_generated_output = discriminator([input_images, gen_output], training=True)\n",
    "\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target_images)\n",
    "    \n",
    "    return gen_total_loss, gen_gan_loss, gen_l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.619255Z",
     "iopub.status.busy": "2022-01-26T04:31:18.618687Z",
     "iopub.status.idle": "2022-01-26T04:31:18.943943Z",
     "shell.execute_reply": "2022-01-26T04:31:18.944367Z"
    },
    "id": "8Fc4NzT-DgEx"
   },
   "outputs": [],
   "source": [
    "for example_input, example_target in test_dataset.take(1):\n",
    "    generate_images(generator, example_input, example_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.949432Z",
     "iopub.status.busy": "2022-01-26T04:31:18.948854Z",
     "iopub.status.idle": "2022-01-26T04:31:18.951199Z",
     "shell.execute_reply": "2022-01-26T04:31:18.951561Z"
    },
    "id": "xNNMDBNH12q-"
   },
   "outputs": [],
   "source": [
    "log_dir= os.path.join(model_path, \"logs\")\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLKOG55MErD0"
   },
   "source": [
    "## Training\n",
    "\n",
    "- For each example input generates an output.\n",
    "- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n",
    "- Next, calculate the generator and the discriminator loss.\n",
    "- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "- Finally, log the losses to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.959097Z",
     "iopub.status.busy": "2022-01-26T04:31:18.958561Z",
     "iopub.status.idle": "2022-01-26T04:31:18.960101Z",
     "shell.execute_reply": "2022-01-26T04:31:18.960467Z"
    },
    "id": "KBKUV2sKXDbY"
   },
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(input_image, target, step):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        gen_output = generator(input_image, training=True)\n",
    "\n",
    "        disc_real_output = discriminator([input_image, target], training=True)\n",
    "        disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
    "        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
    "        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx7s-vBHFKdh"
   },
   "source": [
    "The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n",
    "\n",
    "- Iterates over the number of steps.\n",
    "- Every 10 steps print a dot (`.`).\n",
    "- Every 1k steps: clear the display and run `generate_images` to show the progress.\n",
    "- Every 5k steps: save a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries with the alterations to be used for the data augmentation operations for inputs and outputs\n",
    "\"\"\"\"\n",
    "NOTE: It is important to use exact the same values for the parameters in both dictionaries. \n",
    "The reason why we create 2 dictionaries instead of a common one is because  we want to add the \n",
    "preprocessing function for the output masks (this function sets all the pixel values of the mask to 0 or 1. \n",
    "While the initial ground truth images are binary, after the data augmentation operations, such as rotations and \n",
    "shifts, pixels with intermediate values are appeared due to interpolation)\n",
    "\"\"\"\n",
    "x_datagen_args = dict(\n",
    "    rotation_range=25, \n",
    "    width_shift_range=0.1, \n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect')\n",
    "\n",
    "y_datagen_args = dict(\n",
    "    rotation_range=25, \n",
    "    width_shift_range=0.1, \n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect', \n",
    "    preprocessing_function = lambda x: np.where(x>0, 1, 0).astype(x.dtype))\n",
    "\n",
    "# Instatiate the generators\n",
    "x_datagen = ImageDataGenerator(**x_datagen_args)\n",
    "y_datagen = ImageDataGenerator(**y_datagen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.966760Z",
     "iopub.status.busy": "2022-01-26T04:31:18.966201Z",
     "iopub.status.idle": "2022-01-26T04:31:18.968111Z",
     "shell.execute_reply": "2022-01-26T04:31:18.967696Z"
    },
    "id": "GFyPlBWv1B5j"
   },
   "outputs": [],
   "source": [
    "def fit(train_ds, test_ds, steps):\n",
    "\n",
    "    validate_inputs, validate_targets = next(iter(test_ds.take(1)))\n",
    "   \n",
    "    best_valid_loss = 10**4\n",
    "    previous_model_dir = None\n",
    "    for epoch in range(EPOCHS):\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        #generate_images(generator, example_input, example_target)\n",
    "        r = [np.random.randint(test_set_num)]\n",
    "        r = 5\n",
    "        generate_images(generator, tf.expand_dims(validate_inputs[r], axis=0), tf.expand_dims(validate_targets[r], axis=0))\n",
    "        \n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = validate(generator, discriminator, validate_inputs, validate_targets)\n",
    "        print(f'Epoch {epoch + 1} of {EPOCHS}, gen_total_loss: {gen_total_loss.numpy()}, '\n",
    "              f'gen_gan_loss: {gen_gan_loss.numpy()}, gen_l1_loss: {gen_l1_loss.numpy()}\\n')\n",
    "        for step, (input_image, target) in enumerate(train_ds):\n",
    "                #print(f'Epoch: {epoch + 1}, step: {step}/{steps_per_epoch}')\n",
    "                seed = np.random.randint(10**4)\n",
    "                aug_input = x_datagen.flow(input_image, seed=seed, batch_size=BATCH_SIZE).next()\n",
    "                aug_target = y_datagen.flow(target, seed=seed, batch_size=BATCH_SIZE).next()\n",
    "                train_step(aug_input, aug_target, step)\n",
    "                \n",
    "        if gen_total_loss < best_valid_loss:\n",
    "            best_model_dir = os.path.join(gen_path.split('eps')[0]) + 'eps' + str(epoch + 1)\n",
    "            generator.save(best_model_dir)\n",
    "            print('A new best model was saved')\n",
    "            best_valid_loss = gen_total_loss\n",
    "            \n",
    "            if previous_model_dir:\n",
    "                shutil.rmtree(previous_model_dir)  \n",
    "                previous_model_dir = best_model_dir\n",
    "\n",
    "        # NOTE: I commented the following lines of code, because an error is raised, due to the excessed size of the filepath name\n",
    "        \"\"\"\n",
    "        # Save (checkpoint) the model every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        \"\"\"\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wozqyTh2wmCu"
   },
   "source": [
    "This training loop saves logs that you can view in TensorBoard to monitor the training progress.\n",
    "\n",
    "If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.\n",
    "\n",
    "To launch the viewer paste the following into a code-cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot22ujrlLhOd"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0-8Bzg22ox"
   },
   "source": [
    "Finally, run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:31:18.972385Z",
     "iopub.status.busy": "2022-01-26T04:31:18.971853Z",
     "iopub.status.idle": "2022-01-26T04:56:12.815985Z",
     "shell.execute_reply": "2022-01-26T04:56:12.816411Z"
    },
    "id": "a1zZmKmvOH85",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(train_dataset, test_dataset, steps=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeq9sByu86-B"
   },
   "source": [
    "If you want to share the TensorBoard results _publicly_, you can upload the logs to [TensorBoard.dev](https://tensorboard.dev/) by copying the following into a code-cell.\n",
    "\n",
    "Note: This requires a Google account.\n",
    "\n",
    "```\n",
    "!tensorboard dev upload --logdir {log_dir}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-kT7WHRKz-E"
   },
   "source": [
    "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(gen_path)\n",
    "discriminator.save(os.path.join(gen_path, 'd_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lGhS_LfwQoL"
   },
   "source": [
    "You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
    "\n",
    "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
    "\n",
    "It can also included inline using an `<iframe>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:56:12.821842Z",
     "iopub.status.busy": "2022-01-26T04:56:12.821168Z",
     "iopub.status.idle": "2022-01-26T04:56:12.824115Z",
     "shell.execute_reply": "2022-01-26T04:56:12.823509Z"
    },
    "id": "8IS4c93guQ8E"
   },
   "outputs": [],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
    "    width=\"100%\",\n",
    "    height=\"1000px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMTm4peo3cem"
   },
   "source": [
    "Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:\n",
    "\n",
    "- Check that neither the generator nor the discriminator model has \"won\". If either the `gen_gan_loss` or the `disc_loss` gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
    "- The value `log(2) = 0.69` is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.\n",
    "- For the `disc_loss`, a value below `0.69` means the discriminator is doing better than random on the combined set of real and generated images.\n",
    "- For the `gen_gan_loss`, a value below `0.69` means the generator is doing better than random at fooling the discriminator.\n",
    "- As training progresses, the `gen_l1_loss` should go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:56:12.862731Z",
     "iopub.status.busy": "2022-01-26T04:56:12.828129Z",
     "iopub.status.idle": "2022-01-26T04:56:13.011352Z",
     "shell.execute_reply": "2022-01-26T04:56:13.011730Z"
    },
    "id": "HSSm4kfvJiqv"
   },
   "outputs": [],
   "source": [
    "#!ls {checkpoint_dir}\n",
    "reader = tf.train.load_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.Model()\n",
    "#checkpoint = tf.train.Checkpoint(model)\n",
    "generator = tf.keras.models.load_model('models/256_inputs/f0002/pix2pix/pix2pix_input256_filters64_images561_independent_patches_optAdam_lr0.0002_eps100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:56:13.048242Z",
     "iopub.status.busy": "2022-01-26T04:56:13.016150Z",
     "iopub.status.idle": "2022-01-26T04:56:13.674058Z",
     "shell.execute_reply": "2022-01-26T04:56:13.673578Z"
    },
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# Restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.read(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-26T04:56:13.678803Z",
     "iopub.status.busy": "2022-01-26T04:56:13.678276Z",
     "iopub.status.idle": "2022-01-26T04:56:15.371008Z",
     "shell.execute_reply": "2022-01-26T04:56:15.371407Z"
    },
    "id": "KUgSnmy2nqSP"
   },
   "outputs": [],
   "source": [
    "# Run the trained model on a few examples from the test set\n",
    "for inp, tar in train_dataset.take(5):\n",
    "    generate_images(generator, inp, tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "deepenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5afde5fb8be75fa4232c2400db5e0de612e5b75b692446a76c1449fdda253404"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
